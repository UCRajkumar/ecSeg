{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1568a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import yaml, glob, os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from image_tools import *\n",
    "import seaborn as sns\n",
    "from skimage import *\n",
    "import scipy.stats\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c151cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_sampled_gaussian_kernel(kernel_shape, sigma=1):\n",
    "    if not isinstance(kernel_shape, np.ndarray):\n",
    "        kernel_shape = np.array(kernel_shape)\n",
    "    if not kernel_shape[0] % 2 or not kernel_shape[1]:\n",
    "        print(\"\\n\\tWarning: Even Kernel Used in Convolution\\n\")\n",
    "    \n",
    "    centers = (kernel_shape / 2) - 0.5\n",
    "    kernel_axis_y, kernel_axis_x = [np.arange(kernel_axis_size) - center for kernel_axis_size, center in zip(kernel_shape, centers)]\n",
    "    grid = np.linalg.norm(np.dstack(np.meshgrid(kernel_axis_x, kernel_axis_x)), axis=2).astype(np.float64)\n",
    "    gaussian = scipy.stats.norm.pdf(grid, scale=sigma)\n",
    "    return gaussian / gaussian.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd64035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_proj_kernel(kernel_size, sigma):\n",
    "    g_kernel = scipy_sampled_gaussian_kernel(kernel_size, sigma=sigma)\n",
    "    c_kernel = np.ones(kernel_size)\n",
    "\n",
    "    # Normalizing constant kernel\n",
    "    c_kernel = c_kernel / np.linalg.norm(c_kernel)\n",
    "    \n",
    "    # Projecting and Normalizing gaussian kernel\n",
    "    g_kernel_proj = np.dot(g_kernel.flatten(), c_kernel.flatten()) * c_kernel\n",
    "    g_kernel_perp = g_kernel - g_kernel_proj\n",
    "    g_kernel_perp /= np.linalg.norm(g_kernel_perp)\n",
    "\n",
    "    while len(g_kernel_perp.shape) < 4:\n",
    "        g_kernel_perp = np.expand_dims(g_kernel_perp, -1)\n",
    "    return g_kernel_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cca232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensitivity(I, segmented_cells, intensity_threshold_std_coeff):\n",
    "    # Get Color Sensitivity\n",
    "    seg_copy = segmented_cells.copy().astype(bool).astype(np.float32)\n",
    "    mean = np.array([(seg_copy * I[:,:,chan]).sum() / seg_copy.sum() for chan in range(1, I.shape[-1])])\n",
    "    seg_copy[seg_copy == 0] = np.nan\n",
    "    stdev = np.array([np.nanstd((seg_copy * I[:,:,chan])) for chan in range(1, I.shape[-1])])\n",
    "    color_sensitivity = mean + (intensity_threshold_std_coeff * stdev)\n",
    "    return color_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4f63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholded(I, segmented_cells, g_kernel_perp, normal_threshold, color_sensitivity):\n",
    "    num_channels = I.shape[-1]\n",
    "    inter = np.expand_dims([I[...,channel] for channel in range(1, num_channels)], -1).astype(np.float64)\n",
    "    normal_coefficients = tf.nn.conv2d(inter, g_kernel_perp, strides=1, padding=\"SAME\").eval(session=tf.compat.v1.Session())\n",
    "    assert normal_coefficients.shape[-1] == 1\n",
    "    normal_coefficients = np.dstack(normal_coefficients[...,0])\n",
    "    thresholded = ((normal_coefficients > normal_threshold) * (I[...,1:] > color_sensitivity)).astype(int)\n",
    "    thresholded *= np.dstack([segmented_cells] * (num_channels - 1))\n",
    "    return thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11d65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundaries(s, line_thickness=1):\n",
    "    s = np.expand_dims(s.astype(np.int32), (0, -1))\n",
    "    \n",
    "    lr_kernel = np.array(([1] * line_thickness) + ([-1] * line_thickness))\n",
    "    tb_kernel = np.array(([1] * line_thickness) + ([-1] * line_thickness))\n",
    "\n",
    "    lr_kernel = tf.convert_to_tensor(np.expand_dims(lr_kernel, axis=(0, 2, 3)), dtype=tf.int32)\n",
    "    tb_kernel = tf.convert_to_tensor(np.expand_dims(tb_kernel, axis=(1, 2, 3)), dtype=tf.int32)\n",
    "    \n",
    "    lr_edges = (tf.nn.conv2d(s, lr_kernel, strides=1, padding=\"SAME\").eval(session=tf.compat.v1.Session()) == 0).astype(int)[0]\n",
    "    tb_edges = (tf.nn.conv2d(s, tb_kernel, strides=1, padding=\"SAME\").eval(session=tf.compat.v1.Session()) == 0).astype(int)[0]\n",
    "    boundaries = (lr_edges + tb_edges != 2).astype(int) * 255\n",
    "    \n",
    "    zeros = np.zeros(boundaries.shape).astype(int)\n",
    "    boundaries = np.dstack([boundaries, -boundaries, boundaries])\n",
    "        \n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbf0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_channels(img, aqua_rgb):\n",
    "    if img.shape[-1] == 3:\n",
    "        return img\n",
    "    assert img.shape[-1] == 4\n",
    "    img = img[...,:-1] + np.dstack([coeff * img[...,-1] / 255 for coeff in aqua_rgb[::-1]])\n",
    "    return np.minimum(img, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781f80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_splice_segmentation(i, thresh, s, region):\n",
    "    y_splice, x_splice = region.slice\n",
    "    img_splice = i[y_splice.start:y_splice.stop,x_splice.start:x_splice.stop,:]\n",
    "    thresh_splice = thresh[y_splice.start:y_splice.stop,x_splice.start:x_splice.stop,:]\n",
    "    seg_splice = (s[y_splice.start:y_splice.stop,x_splice.start:x_splice.stop] == region.label).astype(int)\n",
    "    return img_splice, thresh_splice, seg_splice, (y_splice, x_splice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ef5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "\n",
    "    config = open(\"config.yaml\")\n",
    "    var = yaml.load(config, Loader=yaml.FullLoader)['nuclei_fish']\n",
    "    \n",
    "    inpath = var['inpath']\n",
    "    \n",
    "    # Intensity and Normal Distribution Scaled Thresholds\n",
    "    normal_threshold = var['normal_threshold']\n",
    "    intensity_threshold_std_coeff = var['intensity_threshold_std_coeff']\n",
    "    \n",
    "    # Minimum pixels for valid connected component\n",
    "    min_cc_size = var['min_cc_size']\n",
    "    \n",
    "    # Gaussian Kernel Parameters\n",
    "    gaussian_kernel_shape = var['gaussian_kernel_shape']\n",
    "    gaussian_sigma = var['gaussian_sigma']\n",
    "    g_kernel_perp = get_gaussian_proj_kernel(gaussian_kernel_shape, gaussian_sigma)\n",
    "\n",
    "    \n",
    "    # Cosmetic: thickness of segmentation lines\n",
    "    line_thickness = var['line_thickness']\n",
    "    aqua_rgb = [233, 137, 54]\n",
    "    \n",
    "    # NuSeT parameters\n",
    "    bbox_min_score = var['min_score'] \n",
    "    nms_thresh = var['nms_threshold']\n",
    "    resize_scale = var['scale_ratio']\n",
    "    nuclei_size_t = var['nuclei_size_T']\n",
    "\n",
    "    #check input parameters\n",
    "    if(os.path.isdir(os.path.join(inpath)) == False):\n",
    "        print(\"Input folder does not exist. Exiting...\")\n",
    "        sys.exit(2)\n",
    "     \n",
    "    output_folders = ['annotated']\n",
    "    for output_folder in output_folders:\n",
    "        if(os.path.exists(os.path.join(inpath, output_folder))):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(os.path.join(inpath, output_folder))\n",
    "\n",
    "    image_paths = get_imgs(inpath)\n",
    "    first_fish = 'green'\n",
    "    second_fish = 'red'\n",
    "    third_fish = 'aqua'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        sess1, sess2, pred_masks, train_initial, pred_masks_watershed, resize_scale = load_nuset(bbox_min_score, nms_thresh, resize_scale)\n",
    "\n",
    "        dfs = []\n",
    "        for i in image_paths:\n",
    "            path_split = os.path.split(i)\n",
    "            print(\"Processing image: \", i)\n",
    "            img_name = os.path.basename(i)[:-4]\n",
    "            annotated_path = os.path.join(inpath, 'annotated', img_name)\n",
    "            os.makedirs(annotated_path, exist_ok=True)\n",
    "            \n",
    "            if i.endswith('.tif'):\n",
    "                I = u16_to_u8(cv2.imread(i))\n",
    "            elif i.endswith('.npy'):\n",
    "                I = u16_to_u8(np.load(i))\n",
    "            else:\n",
    "                raise AssertionError\n",
    "            blue = I[:,:,0]\n",
    "\n",
    "            segmented_cells = nuclei_segment(blue, resize_scale, sess1, sess2, pred_masks, train_initial, pred_masks_watershed, nuclei_size_t)\n",
    "            segmented_cells_copy = segmented_cells.copy()\n",
    "            \n",
    "            imheight, imwidth = segmented_cells.shape\n",
    "            I = I[:imheight,:imwidth,:]\n",
    "\n",
    "            # Get Color Sensitivity\n",
    "            color_sensitivity = get_sensitivity(I, segmented_cells, intensity_threshold_std_coeff)\n",
    "            \n",
    "            num_channels = I.shape[-1]\n",
    "            \n",
    "            thresholded = get_thresholded(I, segmented_cells, g_kernel_perp, normal_threshold, color_sensitivity)\n",
    "            thresholded_copy = thresholded.copy().astype(np.uint8)\n",
    "\n",
    "            segmented_cells = measure.label(segmented_cells)\n",
    "            regions = measure.regionprops(segmented_cells)\n",
    "    \n",
    "            names = []; cell_sizes = []; centroids = []; \n",
    "            \n",
    "            fish_sizes, fish_blobs, avg_fish, max_fish = [[[] for _ in range(num_channels-1)] for _ in range(4)]\n",
    "            df = pd.DataFrame()\n",
    "            exec_summary = pd.DataFrame()\n",
    "            print('Number of regions: ', len(regions))\n",
    "            \n",
    "            for region in regions:\n",
    "                raw_cell, thresh_cell, cell_seg, (y_splice, x_splice) = cell_splice_segmentation(I, thresholded, segmented_cells, region)\n",
    "                fish = [thresh_cell[...,channel] for channel in range(num_channels-1)]\n",
    "                raw_fish = [raw_cell[...,channel].astype(np.int64) * cell_seg for channel in range(1, num_channels)]\n",
    "                for raw_fish_ch, avg_fish_ch, max_fish_ch, fish_sizes_ch, fish_blobs_ch, fish_splice in zip(raw_fish, avg_fish, max_fish, fish_sizes, fish_blobs, fish):         \n",
    "                    labeled_array, blob_count = scipy.ndimage.measurements.label(fish_splice * cell_seg)\n",
    "                    for blob in measure.regionprops(labeled_array):\n",
    "                        if blob.area < min_cc_size:\n",
    "                            blob_y_splice, blob_x_splice = blob.slice\n",
    "                            component = (labeled_array[blob_y_splice.start:blob_y_splice.stop, blob_x_splice.start:blob_x_splice.stop] == blob.label).astype(int)\n",
    "                            fish_splice[blob_y_splice.start:blob_y_splice.stop, blob_x_splice.start:blob_x_splice.stop] -= 255 * component\n",
    "                            blob_count -= 1\n",
    "                    fish_blobs_ch.append(blob_count)\n",
    "                    fish_pixels = np.sum(fish_splice * cell_seg) / 255\n",
    "                    assert fish_pixels == int(fish_pixels)\n",
    "                    fish_sizes_ch.append(int(fish_pixels))\n",
    "                    avg_fish_intensity, max_fish_intensity = intensity_metrics(raw_fish_ch)\n",
    "                    avg_fish_ch.append(avg_fish_intensity if not np.isnan(avg_fish_intensity) else 0)\n",
    "                    max_fish_ch.append(max_fish_intensity)\n",
    "                \n",
    "                cell_sizes.append(region.area)\n",
    "                center = region.centroid\n",
    "                centroids.append(str(int(center[0])) + '_' + str(int(center[1])))\n",
    "                names.append(path_split[-1][:-4])\n",
    "\n",
    "            df['image_name'] = np.array(names)\n",
    "            df['nucleus_center'] = np.array(centroids)\n",
    "            \n",
    "            for channel_name, fish_sizes_ch, fish_blobs_ch, avg_fish_ch, max_fish_ch in zip((first_fish, second_fish, third_fish), fish_sizes, fish_blobs, avg_fish, max_fish): \n",
    "                df[f'#_FISH_pixels ({channel_name})'] = np.array(fish_sizes_ch)\n",
    "                df[f'#_FISH_blobs ({channel_name})'] = np.array(fish_blobs_ch)\n",
    "                df[f'Avg fish intensity ({channel_name})'] = np.array(avg_fish_ch)\n",
    "                df[f'Max fish intensity ({channel_name})'] = np.array(max_fish_ch)\n",
    "\n",
    "            df['#_DAPI_pixels'] = np.array(cell_sizes)\n",
    "            dfs.append(df)\n",
    "            \n",
    "            thresholds_abbreviation = '_'.join([f\"{letter}{format(x, '.1f')}\" for letter, x in zip(['g', 'r', 'aq'], color_sensitivity)])\n",
    "            image_least_squares_path = f\"{annotated_path}/{img_name}_lsq_n{normal_threshold}_s{min_cc_size}_{thresholds_abbreviation}.tif\"\n",
    "            boundaries = get_boundaries(segmented_cells, line_thickness=line_thickness)\n",
    "            \n",
    "            I = merge_channels(I, aqua_rgb).astype(np.uint8)\n",
    "            img_with_segmentation = np.minimum(I + boundaries, 255).astype(np.uint8)\n",
    "            blob_labeled_img = np.dstack([boundaries[:,:,0], thresholded])\n",
    "            if blob_labeled_img.shape[-1] > 3:\n",
    "                blob_labeled_img = merge_channels(blob_labeled_img, aqua_rgb)\n",
    "            blob_labeled_img = blob_labeled_img.astype(np.uint8)\n",
    "            \n",
    "            assert cv2.imwrite(f\"{annotated_path}/{img_name}_segmentation.tif\", segmented_cells_copy)\n",
    "            assert cv2.imwrite(f\"{annotated_path}/{img_name}_with_segmentation.tif\", img_with_segmentation)\n",
    "            assert cv2.imwrite(f\"{annotated_path}/{img_name}_original.tif\", I)\n",
    "            assert cv2.imwrite(image_least_squares_path, blob_labeled_img)\n",
    "            \n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs.to_csv(os.path.join(path_split[0],  'annotated', 'nuclei_fish_lsq.csv'), index=False)\n",
    "        sess1.close()\n",
    "        sess2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c274173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 17:24:38.158852: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 17:24:38.168775: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3325630000 Hz\n",
      "2023-01-10 17:24:38.170572: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6174800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-10 17:24:38.170610: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/nuset/whole_norm.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./models/nuset/foreground.ckpt\n",
      "Processing image:  /home/giprasad/Owen_ecSeg_Analysis/scratch/Datasets/20200827_MB268/20200827_MB268_MDM4_Red_002.tif\n",
      "Number of regions:  107\n",
      "Processing image:  /home/giprasad/Owen_ecSeg_Analysis/scratch/Datasets/20200827_MB268/20200827_MB268_MDM4_Red_003.tif\n",
      "Number of regions:  136\n",
      "Processing image:  /home/giprasad/Owen_ecSeg_Analysis/scratch/Datasets/20200827_MB268/20200827_MB268_MDM4_Red_001.tif\n",
      "Number of regions:  116\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc0843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecseg",
   "language": "python",
   "name": "ecseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
